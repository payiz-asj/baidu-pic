#!/usr/bin/env python
# -*- coding: utf-8 -*- 
# @Author: Payiz
# @project: ç™¾åº¦å›¾ç‰‡ä¸‹è½½å™¨
# @File : ä¸‹è½½å™¨2.0.py
# @Time : 2020/12/5 05:03:33
# éœ€è¦å®‰è£…ç¬¬ä¸‰æ–¹åº“ï¼Ÿè¯•è¯•ä¸‹é¢è¯­å¥(ä½¿ç”¨æ¸…åé•œåƒæº)
# pip --default-timeout=100 install -i https://pypi.tuna.tsinghua.edu.cn/simple name
# -----------------------------------


import csv
import multiprocessing
import os
import random
import re
import time

import requests
import requests.adapters
# from ä»˜è´¹ip import zhiliu
# from Report_to_wechat.report import send_report
from selenium import webdriver  # ç”¨æ¥é©±åŠ¨æµè§ˆå™¨çš„
from selenium.webdriver.chrome.options import Options  # æµè§ˆå™¨è®¾ç½®
# from selenium.webdriver import ActionChains  # ç ´è§£æ»‘åŠ¨éªŒè¯ç çš„æ—¶å€™ç”¨çš„ å¯ä»¥æ‹–åŠ¨å›¾ç‰‡
from selenium.webdriver.common.action_chains import ActionChains  # é¼ æ ‡æ‚¬åœ
from selenium.webdriver.common.by import By  # å®šä½å™¨ æŒ‰ç…§ä»€ä¹ˆæ–¹å¼æŸ¥æ‰¾ï¼ŒBy.ID,By.CSS_SELECTOR
# åˆ¤æ–­å™¨  å’Œä¸‹é¢WebDriverWaitä¸€èµ·ç”¨çš„
from selenium.webdriver.support import expected_conditions as ec
# é€‰æ‹©å™¨ï¼Œä¸¤ç§æ–¹æ³•ä»»é€‰å…¶ä¸€ï¼Œéƒ½æ˜¯æŒ‡å‘åŒä¸€ä¸ªæ–‡ä»¶
# from selenium.webdriver.support.ui import Select
# é”®ç›˜æ“ä½œ
from selenium.webdriver.support.wait import WebDriverWait  # æµè§ˆå™¨ç­‰å¾…å¯¹è±¡ï¼Œç­‰å¾…é¡µé¢åŠ è½½æŸäº›å…ƒç´ 

# è¯»æ–‡ä»¶ï¼Œå»ºç«‹ç”¨æˆ·è¯·æ±‚å¤´
all_user_agents = []
with open("./è¯·æ±‚å¤´.txt", 'r', encoding='utf-8') as f:
    for i in f:
        all_user_agents.append(i.replace("\n", '').replace('\r', ''))


# è·å–ä¸€ä¸ªå›¾ç‰‡
def get_baidu_pic(keyword, select_size=1, pages=1):
    #########################################################################
    # æ¥å£è¯´æ˜ï¼š
    # keyword                   æœç´¢çš„å›¾ç‰‡å…³é”®è¯
    # select_size               ç­›é€‰å°ºå¯¸ï¼š1 å…¨éƒ¨å°ºå¯¸  2 ç‰¹å¤§  3 å¤§  4 ä¸­  5 å°
    # pages                     çˆ¬å–é¡µæ•°ï¼šä¸€èˆ¬1é¡µå«20-50ä¸ªå›¾ç‰‡
    #########################################################################

    # ç›®æ ‡ç½‘å€
    url = 'https://image.baidu.com/'

    # 0.å»ºç«‹æµè§ˆå™¨å¯¹è±¡ï¼Œå¹¶è®¾ç½®å‚æ•°
    chrome_options = Options()
    # å…³äºçª—å£
    chrome_options.add_argument('--disable-gpu')  # è°·æ­Œæ–‡æ¡£æåˆ°éœ€è¦åŠ ä¸Šè¿™ä¸ªå±æ€§æ¥è§„é¿bug
    # chrome_options.add_argument("--start-maximized")  # çª—å£æœ€å¤§åŒ–
    # chrome_options.add_argument('window-size=100x100')  # è‡ªå®šä¹‰çª—å£å¤§å°
    chrome_options.add_argument('--headless')  # åå°è¿è¡Œ
    # å…³äºé¡µé¢åŠ è½½
    chrome_options.add_argument('--no-sandbox')  # ç¦æ­¢æ¶ˆæ¯æ¡†
    chrome_options.add_argument("--disable-infobars")  # ç¦æ­¢è­¦å‘Šè¯­
    chrome_options.add_argument('--hide-scrollbars')  # éšè—æ»šåŠ¨æ¡, åº”å¯¹ä¸€äº›ç‰¹æ®Šé¡µé¢
    chrome_options.add_argument('blink-settings=imagesEnabled=false')  # ä¸åŠ è½½å›¾ç‰‡, æå‡é€Ÿåº¦
    # å¦ä¸€ç§ä¸åŠ è½½å›¾ç‰‡çš„æ–¹æ³•
    # prefs = {"profile.managed_default_content_settings.images": 2}
    # chrome_options.add_experimental_option("prefs", prefs)
    # å¢åŠ æ‰©å±•
    # chrome_options.add_extension(extension_path)
    # chrome_options.add_argument("--disable-dev-shm-usage") # è¿™ä¸ªä¸çŸ¥é“æ˜¯ä»€ä¹ˆ
    # è®¾ç½®ä¸­æ–‡
    chrome_options.add_argument('lang=zh_CN.UTF-8')

    # æ·»åŠ è¯·æ±‚å¤´å’Œä»£ç†ip
    # global ua
    # global all_ips
    # ip = random.choice(all_ips)
    # chrome_options.add_argument('user-agent=%s'%ua.random())
    # chrome_options.add_argument('--proxy-server='+ip)
    global all_user_agents
    chrome_options.add_argument('user-agent={}'.format(random.choice(all_user_agents)))
    # chrome_options.add_argument('Authorization="token 5325ebf07a0cbebf60780f4ebd016e7b4023065f"')
    # chrome_options.add_argument('Referer="https://statistic.hackathon2020eastchina.top/"')

    # åˆ›å»ºæµè§ˆå™¨å¯¹è±¡
    browser = None
    try:
        browser = webdriver.Chrome(options=chrome_options)
    except Exception as message:
        print('åˆ›å»ºæµè§ˆå™¨å¯¹è±¡å¤±è´¥ï¼Œé”™è¯¯ä»£ç ï¼š', message)

    result = []  # ç»“æœåˆ—è¡¨
    try:
        # 1.æ‰“å¼€ç½‘é¡µ
        browser.get(url)
        time.sleep(0.2)
        # 2.è¾“å…¥éœ€è¦æŸ¥æ‰¾çš„å›¾ç‰‡å
        wait = WebDriverWait(browser, 10, 0.1)
        wait.until(ec.presence_of_all_elements_located((By.ID, "kw")))
        inputs = browser.find_element_by_id("kw")
        inputs.send_keys(keyword)

        # 4.è¾“å…¥å®Œä¿¡æ¯åï¼Œæ‰§è¡ŒæŸ¥è¯¢æ“ä½œ
        # inputs.send_keys(Keys.ENTER)  # 'æŒ‰ä¸‹'å›è½¦é”®ï¼Œè¿™ä¸ªç½‘ç«™æ­¤æ–¹æ³•ä¸è¡Œï¼Œä¼šè·³è½¬åˆ°åˆ«çš„ç½‘ç«™
        browser.find_element_by_class_name("s_newBtn").click()  # ç‚¹å‡»'æŸ¥è¯¢'

        # 5. å¼€å§‹åŠ è½½ç½‘é¡µè·å–ç›®æ ‡å›¾ç‰‡
        try:
            # ç­›é€‰å°ºå¯¸
            # ç­‰å¾…åŠ è½½
            wait = WebDriverWait(browser, 10, 0.1)
            wait.until(ec.presence_of_all_elements_located((By.CLASS_NAME, "sizeFilter")))
            # é¼ æ ‡æ‚¬åœ
            element = browser.find_element_by_class_name('sizeFilter')
            actions = ActionChains(browser)
            actions.move_to_element(element).perform()
            # é€‰æ‹©å°ºå¯¸ï¼Œå¹¶ç‚¹å‡»
            time.sleep(0.3)
            choose_size = browser.find_element_by_xpath(
                '/html/body/div[1]/div[4]/div[2]/div/div[2]/div/div[2]/ul/li[{}]'.format(str(select_size)))
            choose_size.click()
            # åŠ è½½å›¾ç‰‡
            for one_page in range(1, pages + 1):
                # è·å–è¯¥é¡µ
                wait = WebDriverWait(browser, 10, 0.1)
                wait.until(ec.presence_of_all_elements_located((By.ID, "imgid")))
                # time.sleep(0.5)
                # ç­‰å¾…è¯¥é¡µä¸­æ‰€æœ‰å›¾ç‰‡åŠ è½½å®Œæ¯•
                page_item = '//*[@id="imgid"]/div[{}]/ul'.format(str(one_page))
                wait = WebDriverWait(browser, 10, 0.1)
                # å–å¾—è¯¥é¡µæ‰€æœ‰å›¾ç‰‡
                wait.until(ec.presence_of_all_elements_located((By.XPATH, page_item)))
                imgitems = browser.find_element_by_xpath(page_item).find_elements_by_class_name("imgitem")
                # print("ç¬¬{}é¡µå…±æœ‰{}å¼ å›¾ç‰‡ï¼š".format(one_page, len(imgitems)))
                for i in imgitems:
                    title = i.get_attribute('data-title').replace('<strong>', '').replace('</strong>', '')
                    pic_url = i.get_attribute('data-objurl')
                    # print(title, pic_url)
                    result.append({"title": title, "pic_url": pic_url})
                # time.sleep(0.3)
                browser.execute_script('window.scrollTo(0,document.body.scrollHeight)')  # æ»‘åˆ°åº•éƒ¨

        except Exception as message:
            print('å®šä½æŠ¥é”™ï¼š', message)
        finally:
            # source = browser.page_source
            # content = parsel.Selector(source)
            # print('æ‰“å¼€æˆåŠŸï¼š', keyword)
            # # print(source)
            pass

    except Exception as e:
        print('æ‰“å¼€å¤±è´¥ï¼š', keyword)
        print('æ‰“å¼€ç½‘é¡µå¤±è´¥ï¼Œç½‘å€ï¼š{}  é”™è¯¯ä»£ç ï¼š'.format(url), e)
    finally:
        # ç­‰å¾…å‡ ç§’
        # time.sleep(10)
        # å…³é—­æµè§ˆå™¨å¯¹è±¡
        browser.close()
        return result



def error_handler(e):
    print('å¤šè¿›ç¨‹å‡ºé”™å•¦ï¼')
    print(dir(e), "\n")
    print("-->{}<--".format(e.__cause__))

# ä¸‹è½½ä¸€ä¸ªå›¾ç‰‡
def down_load_one_pic(file_path, name, url):
    def validateTitle(title):
        rstr = r"[\/\\\:\*\?\"\<\>\|]"  # '/ \ : * ? " < > |'
        new_title = re.sub(rstr, "_", title)  # æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
        return new_title

    try:
        # è®¾ç½®é‡è¿æ¬¡æ•°
        requests.adapters.DEFAULT_RETRIES = 5
        ses = requests.Session()
        # è®¾ç½®è¿æ¥æ´»è·ƒçŠ¶æ€ä¸ºFalse
        ses.keep_alive = False
        global all_user_agents
        head = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,'
                      '*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
            'Accept-Encoding': 'gzip, deflate',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            # 'Cache-Control': 'max-age=0',
            # 'Connection': 'keep-alive',
            # 'If-Modified-Since': 'Thu, 24 Jan 2019 11:06:45 GMT',
            # 'If-None-Match': '56de0e5b196e494a8dfbe1f638eefd7e',
            # 'Upgrade-Insecure-Requests': '1',
            'user-agent': random.choice(all_user_agents)
        }

        res = requests.get(url, headers=head, timeout=10)
        # print("æ­£åœ¨è®¿é—®ï¼š", name, url)
        if res.status_code == 200 or res.status_code == 403:
            # print(res.text)
            if not os.path.exists(file_path):
                os.makedirs(file_path)
            ss = name if len(name) < 30 else name[0:30]
            ss = validateTitle(ss)
            path = file_path + '/' + ss + '.png'
            with open(path, 'wb') as f:
                f.write(res.content)
            # # with open(file_path, 'a+', newline='', encoding='utf-8-sig')as f:
            #     writer = csv.writer(f)
            #     writer.writerow([prox, origin_ip, where, datetime.datetime.now()])
            print("ç…§ç‰‡", ss, "ä¸‹è½½æˆåŠŸ")
        else:
            print('\n!!! è¯·æ±‚æˆåŠŸï¼Œä½†ç½‘é¡µè¿”å›é”™è¯¯ä»£ç ï¼š{},ç½‘å€{}'.format(str(res.status_code), url))
        # å…³é—­è¯·æ±‚  é‡Šæ”¾å†…å­˜
        res.close()
        del res
    except Exception as e:
        print("\n!!! è¯·æ±‚è¶…å¤±è´¥ï¼Œé”™è¯¯ä»£ç ï¼š{}ï¼Œç½‘å€{}".format(e, url))
        time.sleep(5)


if __name__ == '__main__':
    start = time.time()
    # ç”¨æˆ·äº¤äº’ï¼Œè·å–è¾“å…¥
    keyword = input("è¯·è¾“å…¥éœ€è¦æŸ¥æ‰¾çš„å…³é”®è¯ï¼šè‹¥å¤šæ¡ï¼Œå¯ç”¨ç©ºæ ¼åˆ†å‰²\n").split(' ')
    print(keyword)
    size = int(input("è¯·è¾“å…¥å°ºå¯¸ï¼šå¦‚ 1 å…¨éƒ¨å°ºå¯¸  2 ç‰¹å¤§  3 å¤§  4 ä¸­  5 å° ï¼Œé»˜è®¤1\n"))
    count = int(input("è¯·è¾“å…¥æ•°é‡ï¼šé»˜è®¤ï¼š20\n"))
    # file_path = input("è¯·è¾“å…¥ä¸‹è½½åœ°å€\n")
    file_path = "./ä¸‹è½½ç»“æœ/"
    # å¤„ç†äº¤äº’æ•°æ®
    if keyword == '':
        keyword = "404"    # å“ˆå“ˆå“ˆ
    if size not in [1, 2, 3, 4, 5]:
        size = 1
    if not 0 < count < 1000:
        count = 20
    pages = count // 30 + 1
    # è·å–å›¾ç‰‡
    print("å¼€å§‹è·å–å›¾ç‰‡ï¼Œè¯·ç¨ç­‰ğŸ’–ğŸ’–ğŸ’–")
    all_keywords = []
    for kw in keyword:
        print("æ­£åœ¨è·å–å…³é”®è¯ï¼š ", kw,"  ç›¸å…³çš„å›¾ç‰‡...")
        get_one = {}
        get_one['key_word'] = kw
        get_one['file_path'] = file_path + kw
        get_one['all_pic'] = get_baidu_pic(kw, size, pages)[0:len(keyword)*count]
        all_keywords.append(get_one)

    print("å¼€å§‹ä¸‹è½½å•¦ï¼ğŸ˜˜ğŸ˜˜ğŸ˜˜")
    # æµ‹è¯•ï¼šå•è¿›ç¨‹ä¸‹è½½
    # for i in all_keywords:
    #     print("æ­£åœ¨ä¸‹è½½ï¼š", i['key_word'])
    #     for j in i['all_pic']:
    #         down_load_one_pic(i['file_path'], j['title'], j['pic_url'])
    #     # break
    # # break

    # å¤šè¿›ç¨‹ä¸‹è½½
    params = []
    for one in all_keywords:
        for one_pic in one['all_pic']:
            params.append((one['file_path'], one_pic['title'], one_pic['pic_url']))

    pool = multiprocessing.Pool(processes=32)
    res = pool.starmap_async(func=down_load_one_pic, iterable=params, error_callback=error_handler)
    pool.close()
    pool.join()

    print('\n------------------------------------------------')
    end = time.time()
    print('ç¨‹åºç»“æŸï¼Œæ€»å…±è€—æ—¶ï¼š', end - start, 'ç§’')
    print('\næ¬¢è¿ä¸‹æ¬¡å†æ¥ï¼â¤')
